{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6138892e",
   "metadata": {},
   "source": [
    "Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9d21ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: C:\\Users\\yashk\\Documents\\Projects\\RealWorldProblems\\EcoVision_AI\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define robust paths\n",
    "BASE_DIR = Path(\"..\") # Go up one level from 'notebooks'\n",
    "RAW_DATA_DIR = BASE_DIR / \"data\" / \"raw\"\n",
    "PROCESSED_DATA_DIR = BASE_DIR / \"data\" / \"processed\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project Root: {BASE_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907306db",
   "metadata": {},
   "source": [
    "Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c07ae7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading viratkothari/animal10...\n",
      "Dataset URL: https://www.kaggle.com/datasets/viratkothari/animal10\n",
      "Download complete!\n",
      "Found dataset at: ..\\data\\raw\\Animals-10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# 1. Setup Kaggle Credentials\n",
    "# When you run this, a box will appear. Paste your username and key.\n",
    "# If you don't have them: Go to Kaggle -> Settings -> Create New Token -> Open the downloaded .json file\n",
    "os.environ['KAGGLE_USERNAME'] = getpass.getpass(\"Enter Kaggle Username: \")\n",
    "os.environ['KAGGLE_KEY'] = getpass.getpass(\"Enter Kaggle API Key: \")\n",
    "\n",
    "# 2. Download Dataset using official Kaggle API\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "dataset_name = 'viratkothari/animal10'\n",
    "print(f\"Downloading {dataset_name}...\")\n",
    "\n",
    "# Download and unzip directly to our raw folder\n",
    "api.dataset_download_files(dataset_name, path=RAW_DATA_DIR, unzip=True)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n",
    "# 3. Rename the folder to standardize it\n",
    "# The download likely creates a folder named 'Animals-10' or similar inside raw\n",
    "# Let's find it and rename it to 'animal10' for consistency\n",
    "for item in RAW_DATA_DIR.iterdir():\n",
    "    if item.is_dir() and \"animal\" in item.name.lower():\n",
    "        downloaded_path = item\n",
    "        print(f\"Found dataset at: {downloaded_path}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9d3d27",
   "metadata": {},
   "source": [
    "Cleaning & Integrity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69873905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Complete. Valid: 26179, Deleted Corrupt: 0\n"
     ]
    }
   ],
   "source": [
    "def verify_images(data_dir):\n",
    "    corrupt_count = 0\n",
    "    valid_count = 0\n",
    "    \n",
    "    # Walk through all folders\n",
    "    for filepath in Path(data_dir).rglob(\"*\"):\n",
    "        if filepath.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
    "            try:\n",
    "                # 1. Try opening with PIL (checks header)\n",
    "                img = Image.open(filepath) \n",
    "                img.verify() \n",
    "                \n",
    "                # 2. Try reading with OpenCV (checks pixel data integrity)\n",
    "                # Re-open because verify() closes the file\n",
    "                img_cv = cv2.imread(str(filepath))\n",
    "                if img_cv is None:\n",
    "                    raise ValueError(\"OpenCV could not read\")\n",
    "                \n",
    "                valid_count += 1\n",
    "                \n",
    "            except (IOError, SyntaxError, ValueError) as e:\n",
    "                print(f\"Corrupt file found: {filepath} - {e}\")\n",
    "                os.remove(filepath) # DELETE corrupt file\n",
    "                corrupt_count += 1\n",
    "\n",
    "    print(f\"Scan Complete. Valid: {valid_count}, Deleted Corrupt: {corrupt_count}\")\n",
    "\n",
    "# Run the cleaning\n",
    "verify_images(downloaded_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb2df71",
   "metadata": {},
   "source": [
    "Splitting Data (Train/Val/Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "245daf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed butterfly: 1689 train, 211 val, 212 test\n",
      "Processed cat: 1334 train, 167 val, 167 test\n",
      "Processed chicken: 2478 train, 310 val, 310 test\n",
      "Processed cow: 1492 train, 187 val, 187 test\n",
      "Processed dog: 3890 train, 486 val, 487 test\n",
      "Processed elephant: 1156 train, 145 val, 145 test\n",
      "Processed horse: 2098 train, 262 val, 263 test\n",
      "Processed sheep: 1456 train, 182 val, 182 test\n",
      "Processed spider: 3856 train, 482 val, 483 test\n",
      "Processed squirrel: 1489 train, 186 val, 187 test\n"
     ]
    }
   ],
   "source": [
    "def split_data(source_dir, output_dir, split_ratio=(0.8, 0.1, 0.1)):\n",
    "    # split_ratio = (Train, Validation, Test)\n",
    "    \n",
    "    classes = [d.name for d in source_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    for cls in classes:\n",
    "        # Get all images for this class\n",
    "        cls_path = source_dir / cls\n",
    "        images = [f for f in cls_path.glob('*') if f.suffix.lower() in ['.jpg', '.png', '.jpeg']]\n",
    "        \n",
    "        # Split: Train vs (Val + Test)\n",
    "        train_imgs, temp_imgs = train_test_split(images, train_size=split_ratio[0], random_state=42)\n",
    "        # Split: Val vs Test\n",
    "        val_imgs, test_imgs = train_test_split(temp_imgs, test_size=0.5, random_state=42)\n",
    "        \n",
    "        # Move files to new structure\n",
    "        for subset, img_list in zip(['train', 'val', 'test'], [train_imgs, val_imgs, test_imgs]):\n",
    "            target_dir = output_dir / subset / cls\n",
    "            target_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            for img in img_list:\n",
    "                # We COPY instead of move, to keep raw data safe as backup\n",
    "                shutil.copy(img, target_dir / img.name)\n",
    "                \n",
    "        print(f\"Processed {cls}: {len(train_imgs)} train, {len(val_imgs)} val, {len(test_imgs)} test\")\n",
    "\n",
    "# Execute Split\n",
    "split_data(downloaded_path, PROCESSED_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7691e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
